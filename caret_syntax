################################################
#Models:
#Compiled by:Linlin Cheng
################################################
################################################
library(doMC) #install and register for multicore
registerDoMC(5)

library(caret) 
(confusionMatrix())
library(pROC)
(roc())
################################################
#1.General Caret Models: Two class classification:

MyTrainControl=trainControl(
  method = "repeatedCV",
  number=10,
  repeats=5,
  returnResamp = "all",
  classProbs = TRUE,
  summaryFunction=twoClassSummary
)

model <- train(FL,data=trainset,method='glmnet',
               metric = "ROC",
               tuneGrid = expand.grid(.alpha=c(0,1),.lambda=seq(0,0.05,by=0.01)),
               trControl=MyTrainControl)
model
plot(model, metric='ROC')

####
################################################
################################################
###Tree:
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3,
                       summaryFunction = twoClassSummary,
                       classProbs = TRUE)
set.seed(1)
rpartTune <- train(Class ~ ., data = training, method = "rpart",
                   tuneLength = 30, #automaticaly tune for 30 combos
                   metric = "ROC",
                   trControl = cvCtrl)
plot(rpartTune, scales = list(x = list(log = 10)))
predict(rpartTune$finalModel, newdata, type = "class")
predict(rpartTune, newdata)

rpartProbs <- predict(rpartTune, testing, type = "prob")
 library(pROC)
partROC <- roc(testing$Class, rpartProbs[, "PS"], levels = rev(testProbs$Class))
plot(rpartROC, type = "S", print.thres = .5)






#1. Linear models:
##lm
lmfit <- train(Salary ~., data = train,
               method='lm',
               trControl = fitControl,
               preProc=c('scale', 'center'))
lmfit
lmfit.pred <- predict(lmfit, test)


##Ridge:
fitControl <- trainControl(method = "cv",
                           number = 10)
# Set seq of lambda to test
lambdaGrid <- expand.grid(lambda = 10^seq(10, -2, length=100))

ridge <- train(Salary~., data = train,
               method='ridge',
               trControl = fitControl,
               #                tuneGrid = lambdaGrid
               preProcess=c('center', 'scale')
)

ridge

ridge.pred <- predict(ridge, test)

################################################
################################################
##svm

svmTune <- train(x = trainX,
                 y = training$Class,
                 method = "svmRadial",
                 # The default grid of cost parameters go from 2^-2,
                 # 0.5 to 1,
                 # Well fit 9 values in that sequence via the tuneLength
                 # argument.
                 tuneLength = 9,
                 ## Also add options from preProcess here too
                 preProc = c("center", "scale"),
                 metric = "ROC",
                 trControl = cvCtrl)
svmTune$finalModel
SVM Accuracy Profile
plot(svmTune, metric = "ROC", scales = list(x = list(log =
                                                       2)))
svmPred <- predict(svmTune, testing[, names(testing) != "Class"])
confusionMatrix(svmPred, testing$Class)

################################################
################################################
#xgboost, two class
xgb_grid_1 = expand.grid(
  nrounds = 1000,
  eta = c(0.01, 0.001, 0.0001),
  max_depth = c(2, 4, 6, 8, 10),
  gamma = 1
)

# pack the training control parameters
xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "all",                                                        # save losses across all models
  classProbs = TRUE,                                                           # set to TRUE for AUC to be computed
  summaryFunction = twoClassSummary,
  allowParallel = TRUE
)

# train the model for each parameter combination in the grid, 
#   using CV to evaluate
xgb_train_1 = train(
  x = as.matrix(df_train %>%
                  select(-SeriousDlqin2yrs)),
  y = as.factor(df_train$SeriousDlqin2yrs),
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree"
)

# scatter plot of the AUC against max_depth and eta
ggplot(xgb_train_1$results, aes(x = as.factor(eta), y = max_depth, size = ROC, color = ROC)) + 
  geom_point() + 
  theme_bw() + 
  scale_size_continuous(guide = "none")

################################################
#xgboost, multiclass
tlabel <- as.numeric(as.factor(x_train$label))-1
x_train$label <-tlabel
xgbtrain <- xgb.DMatrix(data.matrix(select(x_train, -label)), label=tlabel, missing=NA)
xgbtest<- xgb.DMatrix(data.matrix(select(x_test, -label)))

xgb_params_1 = list(
  objective = "multi:softmax", 
  num_class = 3,
  eta = 0.1,                                                                  
  max.depth = 1, 
  eval_metric = "merror"                                                          
)

xgbtrain <- xgb.DMatrix(data.matrix(select(x_train, -lable)), label=tlabel, missing=NA)

# set up grid
xgb_grid_1 = expand.grid(
  nrounds = c(100, 500, 1000),
  eta = c(0.3, 0.1, 0.01, 0.001),
  max_depth = c(2, 4, 6, 8, 10),
  gamma = 1,
  colsample_bytree = c(0.1, 0.5),
  min_child_weight = 1 
)

# set up parameters
xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "all",                                                        
  classProbs = TRUE,                                                           
  summaryFunction = multiClassSummary,
  allowParallel = TRUE
)

#tuning script:
xgb_train_1 = train(
  x = data.matrix(x_train %>% select(-lable)),
  y = make.names(as.factor(tlabel)),
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree"
)

xgb_train_1$bestTune

#plug in the best tune:

m = xgb.train(params = list(silent = 1), data = xgbtrain, nrounds = 380,
              eta = 0.07, 
              num_class = 3,                                                                # learning rate
              max.depth = 11, 
              gamma = 1, 
              colsample_bytree = 0.6, 
              min_child_weight = 1,
              eval_metric = "merror",
              objective = "multi:softmax")

p_train = predict(m, newdata = xgbtrain) #prediction
p_test = predict(m, newdata = xgbtest)

################################################
################################################
#random forest
# Random Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(seed)
mtry <- sqrt(ncol(x))
rf_random <- train(Class~., data=dataset, method="rf", metric=metric, tuneLength=15, trControl=control)
print(rf_random)
plot(rf_random)


###
################################################
################################################
###Model Comparison
cvValues <- resamples(list(CART = rpartTune, SVM = svmTune, C5.0 = c5Tune))
summary(cvValues)
xyplot(cvValues, metric = "ROC")
dotplot(cvValues, metric = "ROC")

